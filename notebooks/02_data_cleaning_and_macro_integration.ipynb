{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0f3b202",
   "metadata": {},
   "source": [
    "# Data Cleaning, Validation & Macroeconomic Integration\n",
    "\n",
    "## Objective\n",
    "This notebook cleans and validates loan-level origination and monthly performance data\n",
    "and integrates macroeconomic and capital markets indicators for downstream risk modeling.\n",
    "\n",
    "## Key Components\n",
    "- Loan lifecycle validation (consecutive loan age, valid term structure)\n",
    "- Removal of anomalous loan statuses (e.g., RA)\n",
    "- Feature sanitation and missing value handling\n",
    "- Integration of macro indicators (GDP, CPI, Unemployment, Recession Index)\n",
    "- Integration of capital markets data (SIFMA issuance, outstanding, trading volumes)\n",
    "\n",
    "## Output\n",
    "- Clean loan-level origination and performance datasets\n",
    "- Macro-enriched inputs for feature engineering and ML modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f486daad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Charged off loans with invalid DTI Ratio: 0\n",
      "Paid off loans with invalid DTI Ratio: 0\n",
      "Removed 10769 rows associated with 370 loans due to 'RA' status.\n",
      "Initial rows: 46658, Remaining rows: 35889\n",
      "Initial unique loans: 1974, Remaining unique loans: 1604\n",
      "Loans with zero age: 1270\n",
      "Loans with a recorded ending: 1325\n",
      "Total valid loans: 1263\n",
      "Total invalid loans: 7\n"
     ]
    }
   ],
   "source": [
    "# Import Necessary Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import (confusion_matrix, \n",
    "                             roc_auc_score, \n",
    "                             average_precision_score)\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"distributed.utils_perf\")\n",
    "from multiprocessing import Pool\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "from dask.diagnostics import ProgressBar\n",
    "from collections import Counter\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "df_orig = pd.read_csv(r\"C:\\dwaraka\\github projects\\Risk Analytics\\Freddie_mac_raw_data\\Standard_Quaterly\\sampled_origination_2021_2025Q2.csv\")\n",
    "df_monthly = pd.read_csv(r\"C:\\dwaraka\\github projects\\Risk Analytics\\Freddie_mac_raw_data\\Standard_Quaterly\\sampled_monthly_perf_2021_2025Q2.csv\")\n",
    "\n",
    "\n",
    "\n",
    "charged_loans_indices = set(\n",
    "    df_monthly.loc[df_monthly['Zero Balance Code'].isin([2.0, 3.0, 9.0]), 'Loan Sequence Number']\n",
    ")\n",
    "paid_loans_indices = set(\n",
    "    df_monthly.loc[df_monthly['Zero Balance Code'] == 1.0, 'Loan Sequence Number']\n",
    ")\n",
    "\n",
    "\n",
    "def missing_rate(df):\n",
    "    \"\"\"Returns the missing rate for each column in a dataframe.\"\"\"\n",
    "    return pd.DataFrame(df.isnull().sum() / len(df) * 100)\n",
    "\n",
    "orig_missing_rate = missing_rate(df_orig).rename(columns={0: 'Column Name'})\n",
    "orig_missing_rate\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_orig= df_orig[df_orig['Original Loan Term'] == 360]\n",
    "df_orig = df_orig[df_orig['Credit Score'] != 9999]\n",
    "\n",
    "# Create new binary columns\n",
    "df_orig['Valid DTI Ratio'] = (df_orig['Original Debt-to-Income (DTI) Ratio'] != 999).astype(int)\n",
    "\n",
    "# Filter DataFrame using indices\n",
    "df_charged_off = df_orig[df_orig['Loan Sequence Number'].isin(charged_loans_indices)]\n",
    "df_paid_off = df_orig[df_orig['Loan Sequence Number'].isin(paid_loans_indices)]\n",
    "\n",
    "\n",
    "print(f\"Charged off loans with invalid DTI Ratio: {(df_charged_off['Valid DTI Ratio'] == 0).sum()}\")\n",
    "print(f\"Paid off loans with invalid DTI Ratio: {(df_paid_off['Valid DTI Ratio'] == 0).sum()}\")\n",
    "\n",
    "# Replace values based on conditions\n",
    "df_orig['Super Conforming Flag'].replace({np.nan: '0', 'Y': '1'}, inplace=True)\n",
    "df_orig['HARP Indicator'].replace({np.nan: '0', 'Y': '1'}, inplace=True)\n",
    "df_orig['First Time Homebuyer Flag'].replace({'9':'Unknown'}, inplace=True)\n",
    "df_orig['Program Indicator'].replace({np.nan:'Unknown'}, inplace=True)\n",
    "\n",
    "# Handle missing values\n",
    "df_orig.loc[df_orig['Mortgage Insurance Percentage (MI %)'] == 999, 'Mortgage Insurance Percentage (MI %)'] = np.nan\n",
    "df_orig['Mortgage Insurance Percentage (MI %)'] = df_orig['Mortgage Insurance Percentage (MI %)'].fillna(df_orig['Mortgage Insurance Percentage (MI %)'].median())\n",
    "\n",
    "# Drop specified columns\n",
    "columns_to_drop = [\n",
    "    'Metropolitan Statistical Area (MSA) Or Metropolitan Division', \n",
    "    'Prepayment Penalty Mortgage (PPM) Flag', \n",
    "    'Original Combined Loan-to-Value (CLTV)', \n",
    "    'Amortization Type (Formerly Product Type)', \n",
    "    'Interest Only (I/O) Indicator',\n",
    "    'Pre-HARP Loan Sequence Number',\n",
    "    'Seller Name',\n",
    "    'Servicer Name'\n",
    "]\n",
    "df_orig.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "if 'Original Debt-to-Income (DTI) Ratio' in df_orig.columns:\n",
    "    mean_dti_ratio = df_orig.loc[df_orig['Valid DTI Ratio'] == 0, 'Original Debt-to-Income (DTI) Ratio'].mean()\n",
    "    df_orig.loc[df_orig['Valid DTI Ratio'] == 0, 'Original Debt-to-Income (DTI) Ratio'] = mean_dti_ratio\n",
    "\n",
    "\n",
    "\n",
    "feature_to_keep = [\n",
    "    ## loan\n",
    "    'Loan Sequence Number',\n",
    "    'First Payment Date',\n",
    "    'Original Loan Term',\n",
    "    'Original UPB',\n",
    "    'Mortgage Insurance Percentage (MI %)',\n",
    "    'Original Loan-to-Value (LTV)',\n",
    "    'Original Interest Rate',\n",
    "    'Channel',\n",
    "    'Loan Purpose',\n",
    "    'Super Conforming Flag',\n",
    "    \n",
    "    ## borrower\n",
    "    'Credit Score',\n",
    "    'First Time Homebuyer Flag',\n",
    "    'Original Debt-to-Income (DTI) Ratio',\n",
    "    'Number of Borrowers',\n",
    "    \n",
    "    ## property\n",
    "    'Number of Units',\n",
    "    'Occupancy Status',\n",
    "    'Property State',\n",
    "    'Property Type',\n",
    "    'Property Valuation Method',\n",
    "    \n",
    "    ## Missing value indicator\n",
    "    'Valid DTI Ratio'\n",
    "]\n",
    "\n",
    "df_orig = df_orig.loc[:, feature_to_keep].reset_index(drop=True)\n",
    "\n",
    "month_missing_rate = missing_rate(df_monthly).rename(columns={0: 'Column Name'})\n",
    "month_missing_rate.head()\n",
    "\n",
    "\n",
    "\n",
    "# Before filtering, count the initial number of rows and unique loans\n",
    "initial_row_count = df_monthly.shape[0]\n",
    "initial_loan_count = df_monthly['Loan Sequence Number'].nunique()\n",
    "\n",
    "# Find all 'Loan Sequence Number' values with 'Current Loan Delinquency Status' equal to 'RA'\n",
    "ra_loans = df_monthly[df_monthly['Current Loan Delinquency Status'] == 'RA']['Loan Sequence Number'].unique()\n",
    "\n",
    "# Count how many rows and loans will be removed\n",
    "rows_to_remove = df_monthly[df_monthly['Loan Sequence Number'].isin(ra_loans)].shape[0]\n",
    "loans_to_remove = len(ra_loans)\n",
    "\n",
    "# Then, filter out all rows with these 'Loan Sequence Numbers' from the DataFrame\n",
    "df_monthly = df_monthly[~df_monthly['Loan Sequence Number'].isin(ra_loans)]\n",
    "\n",
    "# After filtering, count the remaining number of rows and unique loans\n",
    "final_row_count = df_monthly.shape[0]\n",
    "final_loan_count = df_monthly['Loan Sequence Number'].nunique()\n",
    "\n",
    "# Replace values based on conditions\n",
    "df_monthly['Delinquency Due to Disaster'].replace({np.nan: 0}, inplace=True)\n",
    "df_monthly.loc[df_monthly['Delinquency Due to Disaster'] == 'Y', 'Delinquency Due to Disaster'] = 1\n",
    "\n",
    "# Replace '0' with integer 0 in 'Current Loan Delinquency Status'\n",
    "df_monthly['Current Loan Delinquency Status'].replace({'0': 0}, inplace=True)\n",
    "\n",
    "# Convert 'Current Loan Delinquency Status' to integer type\n",
    "df_monthly['Current Loan Delinquency Status'] = df_monthly['Current Loan Delinquency Status'].astype(int)\n",
    "\n",
    "# Drop specified columns\n",
    "columns_to_drop_monthly = ['Estimated Loan-to-Value (ELTV)', 'Defect Settlement Date']\n",
    "df_monthly.drop(columns=columns_to_drop_monthly, inplace=True)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Removed {rows_to_remove} rows associated with {loans_to_remove} loans due to 'RA' status.\")\n",
    "print(f\"Initial rows: {initial_row_count}, Remaining rows: {final_row_count}\")\n",
    "print(f\"Initial unique loans: {initial_loan_count}, Remaining unique loans: {final_loan_count}\")\n",
    "\n",
    "missing_rate(df_monthly).rename(columns={0: 'Column Name'})\n",
    "\n",
    "\n",
    "\n",
    "# Identify the unique 'Loan Sequence Number' values present in both dataframes after the operations from the provided codes\n",
    "loan_seq_nums_orig = df_orig['Loan Sequence Number'].unique()\n",
    "loan_seq_nums_monthly = df_monthly['Loan Sequence Number'].unique()\n",
    "\n",
    "# Find the common 'Loan Sequence Number' values between the two dataframes\n",
    "common_loan_seq_nums = set(loan_seq_nums_orig) & set(loan_seq_nums_monthly)\n",
    "\n",
    "\n",
    "# Filter both dataframes to retain only the rows with 'Loan Sequence Number' values present in both dataframes\n",
    "df_orig = df_orig[df_orig['Loan Sequence Number'].isin(common_loan_seq_nums)]\n",
    "df_monthly = df_monthly[df_monthly['Loan Sequence Number'].isin(common_loan_seq_nums)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def filter_and_stats_loans(df_orig, df_monthly):\n",
    "    # Find all loans with 'Loan Age' starting from 0\n",
    "    loans_with_zero_age = set(df_monthly[df_monthly['Loan Age'] == 0]['Loan Sequence Number'].unique())\n",
    "    print(f\"Loans with zero age: {len(loans_with_zero_age)}\")\n",
    "\n",
    "    # Find loans that have a non-NaN 'Zero Balance Code' at least once\n",
    "    loans_with_ending = set(df_monthly.dropna(subset=['Zero Balance Code'])['Loan Sequence Number'].unique())\n",
    "    print(f\"Loans with a recorded ending: {len(loans_with_ending)}\")\n",
    "\n",
    "    # Filter the loans_with_zero_age to only include those that also have a recorded ending\n",
    "    valid_loans = loans_with_zero_age & loans_with_ending\n",
    "\n",
    "    # Filter df_monthly to only include loans with zero age and a recorded ending\n",
    "    df_monthly_filtered = df_monthly[df_monthly['Loan Sequence Number'].isin(valid_loans)]\n",
    "\n",
    "    # Check if ages are consecutive for each loan\n",
    "    consecutively_valid_loans = set()\n",
    "    for loan, group in df_monthly_filtered.groupby('Loan Sequence Number'):\n",
    "        if list(group['Loan Age']) == list(range(len(group))):\n",
    "            consecutively_valid_loans.add(loan)\n",
    "\n",
    "    # Define valid loans as those with consecutive ages and a recorded ending\n",
    "    valid_loans = valid_loans & consecutively_valid_loans\n",
    "\n",
    "    # Define invalid loans\n",
    "    invalid_loans = loans_with_zero_age - valid_loans\n",
    "    \n",
    "    # Remove invalid loans from dataframes\n",
    "    df_orig_filtered = df_orig[df_orig['Loan Sequence Number'].isin(valid_loans)]\n",
    "    df_monthly_filtered = df_monthly_filtered[df_monthly_filtered['Loan Sequence Number'].isin(valid_loans)]\n",
    "    \n",
    "    # Output statistics\n",
    "    print(f\"Total valid loans: {len(valid_loans)}\")\n",
    "    print(f\"Total invalid loans: {len(invalid_loans)}\")\n",
    "    \n",
    "    return df_orig_filtered, df_monthly_filtered\n",
    "\n",
    "# Usage\n",
    "df_orig_valid, df_monthly_valid = filter_and_stats_loans(df_orig, df_monthly)\n",
    "\n",
    "\n",
    "df_orig_valid.to_csv(r\"C:\\dwaraka\\github projects\\Risk Analytics\\Freddie_mac_raw_data\\Standard_Quaterly\\orig_valid.csv\", index=False)\n",
    "df_monthly_valid.to_csv(r\"C:\\dwaraka\\github projects\\Risk Analytics\\Freddie_mac_raw_data\\Standard_Quaterly\\monthly_valid.csv\", index=False)\n",
    "\n",
    "\n",
    "# np.random.seed(42)\n",
    "# rand_loan_idx = np.random.choice(np.array(list(selected_loan_idxes)), 1)[0]\n",
    "rand_loan_idx = df_orig_valid.sample(1, random_state=42)['Loan Sequence Number'].values[0]\n",
    "df_orig_valid.loc[df_orig_valid['Loan Sequence Number']==rand_loan_idx, :]\n",
    "\n",
    "\n",
    "\n",
    "# Define paths for the CSV files on the base path \n",
    "orig_csv_path = r\"C:\\dwaraka\\github projects\\Risk Analytics\\Freddie_mac_raw_data\\Standard_Quaterly\\df_orig.csv\"\n",
    "monthly_csv_path = r\"C:\\dwaraka\\github projects\\Risk Analytics\\Freddie_mac_raw_data\\Standard_Quaterly\\df_monthly.csv\"\n",
    "\n",
    "# Output dataframes to CSV\n",
    "df_orig_valid.to_csv(orig_csv_path, index=False)\n",
    "df_monthly_valid.to_csv(monthly_csv_path, index=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4dbd4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year        UST          MBS  Corporates     Munis       Agency        ABS  \\\n",
      "0  2014  12504.782  8841.996778    7465.750  3865.665  2028.749936  1349.3802   \n",
      "1  2015  13191.555  8894.813341    7704.250  3892.677  1995.396249  1376.5868   \n",
      "2  2016  13908.241  9023.211929    7956.764  3947.421  1971.692165  1391.7603   \n",
      "3  2017  14468.781  9304.524737    8305.107  3968.518  1934.670761  1457.9101   \n",
      "4  2018  15607.967  9732.329779    8505.424  3926.704  1842.577463  1615.6286   \n",
      "\n",
      "           CP         Total  \n",
      "0  930.385883  36986.709797  \n",
      "1  941.492441  37996.770831  \n",
      "2  884.870210  39083.960604  \n",
      "3  965.932706  40405.444304  \n",
      "4  995.971356  42226.602198  \n",
      "   Year  Treasury Securities  Mortgage-Related Securities  \\\n",
      "0  2015          2122.517098                  1800.703618   \n",
      "1  2016          2169.443414                  2044.186751   \n",
      "2  2017          2224.339460                  2003.380375   \n",
      "3  2018          2684.657405                  1872.952098   \n",
      "4  2019          2935.479517                  2118.933939   \n",
      "\n",
      "   Corporate Securities  Municipal Securities  Federal Agency Securities  \\\n",
      "0            1531.14751             405.11240                 645.476300   \n",
      "1            1564.70937             451.92965                 927.870876   \n",
      "2            1686.25949             449.00159                 731.313675   \n",
      "3            1390.94415             346.85794                 653.588000   \n",
      "4            1464.28124             426.37442                 989.328960   \n",
      "\n",
      "   Asset-Backed Securities  Issuance (Total)  \n",
      "0               333.397408       6838.354333  \n",
      "1               325.419729       7483.559790  \n",
      "2               550.295326       7644.589916  \n",
      "3               516.924613       7465.924207  \n",
      "4               434.656335       8369.054412  \n",
      "   Year         UST  Agency MBS  Non-Agency MBS  Corporates     Munis  \\\n",
      "0  2015  490.030000  192.262348        3.067142   29.129599   8.61541   \n",
      "1  2016  519.100000  209.483738        2.785053   31.150977  11.05823   \n",
      "2  2017  505.160000  208.734879        2.297960   32.173970  10.75981   \n",
      "3  2018  547.800000  218.958848        1.606868   32.828372  11.60975   \n",
      "4  2019  624.642857  248.964161        1.430860   35.591135  11.49593   \n",
      "\n",
      "   Agency MBS       ABS       Total  \n",
      "0  192.262348  1.439268  729.792909  \n",
      "1  209.483738  1.330264  780.312415  \n",
      "2  208.734879  1.416964  764.695777  \n",
      "3  218.958848  1.423158  817.717364  \n",
      "4  248.964161  1.530379  927.818405  \n",
      "(11, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>observation_date</th>\n",
       "      <th>UNRATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1948-01-01</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1948-02-01</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1948-03-01</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1948-04-01</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1948-05-01</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>2025-02-01</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>2025-03-01</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>928</th>\n",
       "      <td>2025-05-01</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929</th>\n",
       "      <td>2025-06-01</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>930 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    observation_date  UNRATE\n",
       "0         1948-01-01     3.4\n",
       "1         1948-02-01     3.8\n",
       "2         1948-03-01     4.0\n",
       "3         1948-04-01     3.9\n",
       "4         1948-05-01     3.5\n",
       "..               ...     ...\n",
       "925       2025-02-01     4.1\n",
       "926       2025-03-01     4.2\n",
       "927       2025-04-01     4.2\n",
       "928       2025-05-01     4.2\n",
       "929       2025-06-01     4.1\n",
       "\n",
       "[930 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Load Excel\n",
    "df = pd.read_excel(r\"C:\\dwaraka\\github projects\\Risk Analytics\\Economic Data\\SIFMA\\2025Q2\\US-Fixed-Income-Securities-Statistics-SIFMA.xlsx\", sheet_name='Outstanding')\n",
    "\n",
    "# Set header from row 6 (index 6)\n",
    "df.columns = df.iloc[6]\n",
    "df = df.iloc[7:].copy()\n",
    "\n",
    "# Strip whitespace and convert columns to string\n",
    "df.columns = df.columns.astype(str).str.strip()\n",
    "\n",
    "# Manually deduplicate columns\n",
    "def dedup_columns(cols):\n",
    "    seen = {}\n",
    "    new_cols = []\n",
    "    for col in cols:\n",
    "        if col not in seen:\n",
    "            seen[col] = 0\n",
    "            new_cols.append(col)\n",
    "        else:\n",
    "            seen[col] += 1\n",
    "            new_cols.append(f\"{col}_{seen[col]}\")\n",
    "    return new_cols\n",
    "\n",
    "df.columns = dedup_columns(df.columns)\n",
    "\n",
    "# Rename first column as 'Year'\n",
    "df.rename(columns={df.columns[0]: 'Year'}, inplace=True)\n",
    "\n",
    "\n",
    "# Keep only rows where Year is a 4-digit number\n",
    "df = df[df['Year'].astype(str).str.match(r'^\\d{4}$')]\n",
    "\n",
    "# Define relevant base columns\n",
    "base_cols = ['Year', 'UST', 'MBS', 'Corporates', 'Munis', 'Agency', 'ABS', 'CP', 'Total']\n",
    "\n",
    "\n",
    "# Keep only one instance of each base column\n",
    "main_cols = []\n",
    "for col in base_cols:\n",
    "    for c in df.columns:\n",
    "        if c.startswith(col):\n",
    "            main_cols.append(c)\n",
    "            break\n",
    "\n",
    "df = df[main_cols].copy()\n",
    "\n",
    "# Convert numeric columns\n",
    "for col in df.columns:\n",
    "    if col != 'Year':\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Reset index\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Final preview\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "df_sifma_market = pd.read_excel(r\"C:\\dwaraka\\github projects\\Risk Analytics\\Economic Data\\SIFMA\\2025Q2\\Market_size.xlsx\")\n",
    "    \n",
    "df_sifma_market\n",
    "\n",
    "# Load the Excel sheet\n",
    "df_sifma_issu = pd.read_excel(\n",
    "    r\"C:\\dwaraka\\github projects\\Risk Analytics\\Economic Data\\SIFMA\\2025Q2\\US-Fixed-Income-Securities-Statistics-SIFMA.xlsx\",\n",
    "    sheet_name='Issuance'\n",
    ")\n",
    "\n",
    "\n",
    "# Set header from row 6 (index 6)\n",
    "df_sifma_issu.columns = df_sifma_issu.iloc[6]\n",
    "df_sifma_issu = df_sifma_issu.iloc[7:].copy()\n",
    "\n",
    "# Clean column names\n",
    "df_sifma_issu.columns = df_sifma_issu.columns.astype(str).str.strip()\n",
    "\n",
    "# Deduplicate column names\n",
    "def dedup_columns(cols):\n",
    "    seen = {}\n",
    "    new_cols = []\n",
    "    for col in cols:\n",
    "        if col not in seen:\n",
    "            seen[col] = 0\n",
    "            new_cols.append(col)\n",
    "        else:\n",
    "            seen[col] += 1\n",
    "            new_cols.append(f\"{col}_{seen[col]}\")\n",
    "    return new_cols\n",
    "\n",
    "df_sifma_issu.columns = dedup_columns(df_sifma_issu.columns)\n",
    "\n",
    "# Rename first column to 'Year'\n",
    "df_sifma_issu.rename(columns={df_sifma_issu.columns[0]: 'Year'}, inplace=True)\n",
    "\n",
    "# Filter for rows where 'Year' is a 4-digit number\n",
    "df_sifma_issu = df_sifma_issu[df_sifma_issu['Year'].astype(str).str.match(r'^\\d{4}$')]\n",
    "\n",
    "# Define base columns of interest\n",
    "base_cols = ['Year', 'UST', 'MBS', 'Corporates', 'Munis', 'Agency', 'ABS', 'Total']\n",
    "\n",
    "# Keep only the first instance of each relevant column\n",
    "main_cols = []\n",
    "for col in base_cols:\n",
    "    for c in df_sifma_issu.columns:\n",
    "        if c.startswith(col):\n",
    "            main_cols.append(c)\n",
    "            break\n",
    "\n",
    "df_sifma_issu = df_sifma_issu[main_cols].copy()\n",
    "\n",
    "# Convert relevant columns to numeric\n",
    "for col in df_sifma_issu.columns:\n",
    "    if col != 'Year':\n",
    "        df_sifma_issu[col] = pd.to_numeric(df_sifma_issu[col], errors='coerce')\n",
    "\n",
    "# Rename columns for clarity\n",
    "rename_dict = {\n",
    "    \"Corporates\": \"Corporate Securities\",\n",
    "    \"MBS\": \"Mortgage-Related Securities\",\n",
    "    \"ABS\": \"Asset-Backed Securities\",\n",
    "    \"Agency\": \"Federal Agency Securities\",\n",
    "    \"UST\": \"Treasury Securities\",\n",
    "    \"Munis\": \"Municipal Securities\",\n",
    "    \"Total\": \"Issuance (Total)\"\n",
    "}\n",
    "df_sifma_issu.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "# Reset index\n",
    "df_sifma_issu = df_sifma_issu.reset_index(drop=True)\n",
    "\n",
    "# Preview\n",
    "print(df_sifma_issu.head())\n",
    "\n",
    "\n",
    "\n",
    "# Load the Excel sheet\n",
    "df_sifma_trade = pd.read_excel(\n",
    "    r\"C:\\dwaraka\\github projects\\Risk Analytics\\Economic Data\\SIFMA\\2025Q2\\US-Fixed-Income-Securities-Statistics-SIFMA.xlsx\",\n",
    "    sheet_name='Trading Volume'\n",
    ")\n",
    "\n",
    "# Set header from row 6 (index 6)\n",
    "df_sifma_trade.columns = df_sifma_trade.iloc[6]\n",
    "df_sifma_trade = df_sifma_trade.iloc[7:].copy()\n",
    "\n",
    "# Clean column names\n",
    "df_sifma_trade.columns = df_sifma_trade.columns.astype(str).str.strip()\n",
    "\n",
    "# Deduplicate column names\n",
    "def dedup_columns(cols):\n",
    "    seen = {}\n",
    "    new_cols = []\n",
    "    for col in cols:\n",
    "        if col not in seen:\n",
    "            seen[col] = 0\n",
    "            new_cols.append(col)\n",
    "        else:\n",
    "            seen[col] += 1\n",
    "            new_cols.append(f\"{col}_{seen[col]}\")\n",
    "    return new_cols\n",
    "\n",
    "df_sifma_trade.columns = dedup_columns(df_sifma_trade.columns)\n",
    "\n",
    "# Rename first column to 'Year'\n",
    "df_sifma_trade.rename(columns={df_sifma_trade.columns[0]: 'Year'}, inplace=True)\n",
    "\n",
    "# Keep only rows where Year is a 4-digit number\n",
    "df_sifma_trade = df_sifma_trade[df_sifma_trade['Year'].astype(str).str.match(r'^\\d{4}$')]\n",
    "\n",
    "# Define expected base columns\n",
    "base_cols = ['Year', 'UST', 'Agency MBS', 'Non-Agency MBS', 'Corporates', 'Munis', 'Agency', 'ABS', 'Total']\n",
    "\n",
    "# Select only the first instance of each matching column\n",
    "main_cols = []\n",
    "for col in base_cols:\n",
    "    for c in df_sifma_trade.columns:\n",
    "        if c.startswith(col):\n",
    "            main_cols.append(c)\n",
    "            break\n",
    "\n",
    "df_sifma_trade = df_sifma_trade[main_cols].copy()\n",
    "\n",
    "# Convert numeric columns safely\n",
    "for col in df_sifma_trade.columns:\n",
    "    if col != 'Year' and df_sifma_trade[col].ndim == 1:\n",
    "        df_sifma_trade[col] = pd.to_numeric(df_sifma_trade[col], errors='coerce')\n",
    "\n",
    "# Reset index\n",
    "df_sifma_trade = df_sifma_trade.reset_index(drop=True)\n",
    "\n",
    "# Final preview\n",
    "print(df_sifma_trade.head())\n",
    "print(df_sifma_trade.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_gdp = pd.read_csv(r\"C:\\dwaraka\\github projects\\Risk Analytics\\Economic Data\\Macro\\GDP.csv\")\n",
    "df_gdp \n",
    "\n",
    "\n",
    "df_recession = pd.read_csv(r\"C:\\dwaraka\\github projects\\Risk Analytics\\Economic Data\\Macro\\GDP-based Recession index.csv\")\n",
    "df_recession \n",
    "\n",
    "\n",
    "df_cpi = pd.read_csv(r\"C:\\dwaraka\\github projects\\Risk Analytics\\Economic Data\\Macro\\cpi.csv\")\n",
    "df_cpi \n",
    "\n",
    "\n",
    "df_YC = pd.read_csv(r\"C:\\dwaraka\\github projects\\Risk Analytics\\Economic Data\\Macro\\T10Y2Y.csv\")\n",
    "df_YC\n",
    "\n",
    "\n",
    "\n",
    "df_housing_price = pd.read_csv(r\"C:\\dwaraka\\github projects\\Risk Analytics\\Economic Data\\Macro\\fmhpi_master_file.csv\")\n",
    "df_housing_price \n",
    "\n",
    "\n",
    "\n",
    "df_unemployment = pd.read_csv(r\"C:\\dwaraka\\github projects\\Risk Analytics\\Economic Data\\Macro\\UNRATE.csv\")\n",
    "df_unemployment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "647737ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data file loaded: 310 unique series IDs\n",
      "Matching series IDs between data & series metadata: 310\n",
      "Rows after merging data + filtered series: 184140\n",
      "Final merged rows: 184140\n",
      "✅ Saved to C:\\dwaraka\\github projects\\Risk Analytics\\Economic Data\\LAUS\\laus_filtered_monthly_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# ===== 0. Settings =====\n",
    "target_area_type = 'A'  # 'A'=Statewide, 'B'=MSA, etc.\n",
    "\n",
    "# ===== 1. File paths =====\n",
    "data_file   = r\"C:\\dwaraka\\github projects\\Risk Analytics\\Economic Data\\LAUS\\la.data.3.AllStatesS\"\n",
    "series_file = r\"C:\\dwaraka\\github projects\\Risk Analytics\\Economic Data\\LAUS\\la.series\"\n",
    "area_file   = r\"C:\\dwaraka\\github projects\\Risk Analytics\\Economic Data\\LAUS\\la.area\"\n",
    "measure_file= r\"C:\\dwaraka\\github projects\\Risk Analytics\\Economic Data\\LAUS\\la.measure\"\n",
    "srd_file    = r\"C:\\dwaraka\\github projects\\Risk Analytics\\Economic Data\\LAUS\\la.state_region_division\"\n",
    "\n",
    "# ===== 2. Load main statewide data =====\n",
    "# This file is TAB-separated with a real header, so:\n",
    "df_data = pd.read_csv(data_file, sep='\\t', dtype=str)\n",
    "\n",
    "# Ensure clean columns\n",
    "df_data.columns = df_data.columns.str.strip()\n",
    "for c in df_data.columns:\n",
    "    df_data[c] = df_data[c].astype(str).str.strip()\n",
    "\n",
    "# Keep only monthly rows (M01..M12)\n",
    "df_data = df_data[df_data['period'].str.match(r'^M\\d{2}$')].copy()\n",
    "\n",
    "# Convert columns\n",
    "df_data['month'] = df_data['period'].str[1:].astype(int)\n",
    "df_data['year']  = df_data['year'].astype(int)\n",
    "df_data['value'] = pd.to_numeric(df_data['value'], errors='coerce')\n",
    "\n",
    "print(f\"Data file loaded: {df_data['series_id'].nunique()} unique series IDs\")\n",
    "\n",
    "# ===== 3. Load metadata files =====\n",
    "df_series  = pd.read_csv(series_file, sep='\\t', dtype=str)\n",
    "df_area    = pd.read_csv(area_file, sep='\\t', dtype=str)\n",
    "df_measure = pd.read_csv(measure_file, sep='\\t', dtype=str)\n",
    "df_srd     = pd.read_csv(srd_file, sep='\\t', dtype=str, names=['srd_code','state_name'])\n",
    "\n",
    "# Strip spaces\n",
    "for df in [df_series, df_area, df_measure, df_srd]:\n",
    "    df.columns = df.columns.str.strip()\n",
    "    for c in df.columns:\n",
    "        df[c] = df[c].astype(str).str.strip()\n",
    "\n",
    "# ===== 4. Filter series by target area type =====\n",
    "target_area_codes = df_area[df_area['area_type_code'] == target_area_type]['area_code']\n",
    "df_series_filtered = df_series[df_series['area_code'].isin(target_area_codes)].copy()\n",
    "\n",
    "# Debug: Check overlap of series IDs\n",
    "overlap_series = set(df_data['series_id']) & set(df_series_filtered['series_id'])\n",
    "print(f\"Matching series IDs between data & series metadata: {len(overlap_series)}\")\n",
    "\n",
    "# ===== 5. Merge steps =====\n",
    "merge1 = df_data.merge(\n",
    "    df_series_filtered[['series_id','area_code','measure_code','srd_code']],\n",
    "    on='series_id', how='inner'\n",
    ")\n",
    "print(f\"Rows after merging data + filtered series: {merge1.shape[0]}\")\n",
    "\n",
    "merge2 = merge1.merge(df_area[['area_code','area_text','area_type_code']], \n",
    "                      on='area_code', how='left')\n",
    "merge3 = merge2.merge(df_measure, on='measure_code', how='left')\n",
    "merge4 = merge3.merge(df_srd, on='srd_code', how='left')  # state/region text\n",
    "\n",
    "# Prefer area_text, fallback to state_name from srd\n",
    "merge4['area_name'] = merge4['area_text']\n",
    "merge4.loc[merge4['area_name'].isna() | (merge4['area_name'] == ''), 'area_name'] = merge4['state_name']\n",
    "\n",
    "print(f\"Final merged rows: {merge4.shape[0]}\")\n",
    "\n",
    "# ===== 6. Pivot to wide format =====\n",
    "out = merge4.pivot_table(\n",
    "    index=['area_code','area_name','year','month'],\n",
    "    columns='measure_text', values='value'\n",
    ").reset_index()\n",
    "\n",
    "# Rename key columns if present\n",
    "rename_map = {\n",
    "    'employment': 'Employed',\n",
    "    'unemployment': 'Unemployed',\n",
    "    'unemployment rate': '% Unemployed',\n",
    "    'labor force': 'Labor Force',\n",
    "    'population': 'Population'\n",
    "}\n",
    "out = out.rename(columns={k: v for k,v in rename_map.items() if k in out.columns})\n",
    "\n",
    "# ===== 7. Save =====\n",
    "output_path = r\"C:\\dwaraka\\github projects\\Risk Analytics\\Economic Data\\LAUS\\laus_filtered_monthly_cleaned.csv\"\n",
    "out.to_csv(output_path, index=False)\n",
    "print(f\"✅ Saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch-test)",
   "language": "python",
   "name": "torch-test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
